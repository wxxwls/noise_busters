{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4e2ed6530ded4ef78398b8358bc477ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4eb1867b557e459a850eebdcfb566e64",
              "IPY_MODEL_b1561d7757e24c2f9f16458375ed38bb",
              "IPY_MODEL_dcfd5e2d148b4f4aaafbde6eae156992"
            ],
            "layout": "IPY_MODEL_3d717e3048a3444da96483d40e0ae1e4"
          }
        },
        "4eb1867b557e459a850eebdcfb566e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_683133588445473f8586d6ac72de56eb",
            "placeholder": "​",
            "style": "IPY_MODEL_e9081841309f4398bc7f657dea16eb48",
            "value": "model.safetensors: 100%"
          }
        },
        "b1561d7757e24c2f9f16458375ed38bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b7871608f654efa981faf7a24aee05f",
            "max": 114374272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a35e3841aa041aeaa58ff720eb65f9d",
            "value": 114374272
          }
        },
        "dcfd5e2d148b4f4aaafbde6eae156992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65ebb1fc5cbe45699e0d5d2fbe250a05",
            "placeholder": "​",
            "style": "IPY_MODEL_f04d2e75faa8481f91007bc5ab50d320",
            "value": " 114M/114M [00:00&lt;00:00, 259MB/s]"
          }
        },
        "3d717e3048a3444da96483d40e0ae1e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "683133588445473f8586d6ac72de56eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9081841309f4398bc7f657dea16eb48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b7871608f654efa981faf7a24aee05f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a35e3841aa041aeaa58ff720eb65f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65ebb1fc5cbe45699e0d5d2fbe250a05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f04d2e75faa8481f91007bc5ab50d320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, json, random, re\n",
        "from pathlib import Path\n",
        "from hashlib import md5\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "CFG = {\n",
        "    \"DATA_ROOT\": \"/content/drive/MyDrive/kaggle_noise\",  # ★ 경로 수정\n",
        "    \"SR\": 22050,\n",
        "    \"DURATION\": 2.5,\n",
        "    \"N_MELS\": 128, \"FMIN\": 20, \"FMAX\": 8000,\n",
        "    \"N_FFT\": 2048, \"HOP\": 512,\n",
        "    \"SEED\": 42,\n",
        "    \"PLOT_EDA\": True,\n",
        "}\n",
        "CACHE_NAME  = \"logmel_cache_np\"\n",
        "CSV_PREFIX  = \"ast_split_with_spec\"\n",
        "\n",
        "# -------- Colab Drive (무시 가능) --------\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "random.seed(CFG[\"SEED\"]); np.random.seed(CFG[\"SEED\"])\n",
        "\n",
        "# ---------------- CSV 로드 ----------------\n",
        "DATA_ROOT = Path(CFG[\"DATA_ROOT\"])\n",
        "for n in [\"train_split.csv\",\"val_split.csv\",\"test_split.csv\"]:\n",
        "    assert (DATA_ROOT/n).exists(), f\"{n} 가 필요합니다. ({DATA_ROOT/n})\"\n",
        "\n",
        "train_src = pd.read_csv(DATA_ROOT/\"train_split.csv\")\n",
        "val_src   = pd.read_csv(DATA_ROOT/\"val_split.csv\")\n",
        "test_src  = pd.read_csv(DATA_ROOT/\"test_split.csv\")\n",
        "\n",
        "# =========================\n",
        "# 파일명 → 메타 파싱 (예: 2F_book_1_3_2.wav)\n",
        "# =========================\n",
        "def parse_meta_from_filename(wav_name:str):\n",
        "    nm = Path(wav_name).stem\n",
        "    parts = nm.split(\"_\")\n",
        "    floor, source, intensity, distance, repeat = None, None, None, None, None\n",
        "    if len(parts) >= 5:\n",
        "        p0 = parts[0].upper()\n",
        "        m = re.match(r\"(\\d+)\\s*F$\", p0)\n",
        "        if m: floor = int(m.group(1))\n",
        "        else:\n",
        "            try: floor = int(p0)\n",
        "            except: floor = None\n",
        "        source = parts[1]\n",
        "        try: intensity = float(parts[2])\n",
        "        except: intensity = None\n",
        "        try: distance  = float(parts[3])\n",
        "        except: distance = None\n",
        "        try: repeat    = int(parts[4])\n",
        "        except: repeat = None\n",
        "    return floor, source, intensity, distance, repeat\n",
        "\n",
        "# =========================\n",
        "# 경로 자동 보정\n",
        "# =========================\n",
        "def _try_candidates(split:str, category:str, wav_file:str, wav_path:str, data_root:Path):\n",
        "    category = (category or \"\").strip()\n",
        "    wav_file = (wav_file or \"\").strip()\n",
        "    wav_path = (wav_path or \"\").lstrip(\"/\") if isinstance(wav_path, str) else \"\"\n",
        "    candidates = []\n",
        "    if wav_path:\n",
        "        candidates.append(data_root / split / \"data\" / wav_path)\n",
        "    if category and wav_file:\n",
        "        candidates.append(data_root / split / \"data\" / category / wav_file)\n",
        "    if wav_path and \"/\" in wav_path and wav_path.split(\"/\", 1)[0] in (\"2F\", \"3F\"):\n",
        "        no_floor = wav_path.split(\"/\", 1)[1]\n",
        "        candidates.append(data_root / split / \"data\" / no_floor)\n",
        "    if category and wav_file:\n",
        "        for fl in (\"2F\",\"3F\"):\n",
        "            candidates.append(data_root / split / \"data\" / fl / category / wav_file)\n",
        "    if wav_path and \"/\" in wav_path and wav_path.split(\"/\",1)[0] not in (\"2F\",\"3F\"):\n",
        "        for fl in (\"2F\",\"3F\"):\n",
        "            candidates.append(data_root / split / \"data\" / fl / wav_path)\n",
        "    for p in candidates:\n",
        "        if p.exists(): return str(p)\n",
        "    return None\n",
        "\n",
        "def normalize_split(df: pd.DataFrame, split: str, data_root: Path) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "\n",
        "    # db\n",
        "    col_db = \"db\" if \"db\" in df.columns else (\"decibel\" if \"decibel\" in df.columns else None)\n",
        "    if col_db is None: raise AssertionError(\"CSV에 db(또는 decibel) 컬럼이 필요합니다.\")\n",
        "    def _to_float_db(s):\n",
        "        try: return float(str(s).replace(\",\", \"\").strip())\n",
        "        except: return np.nan\n",
        "    db = df[col_db].map(_to_float_db).astype(np.float32)\n",
        "\n",
        "    # cls/source\n",
        "    if \"cls\" in df.columns: cls = df[\"cls\"].astype(str)\n",
        "    elif \"category\" in df.columns: cls = df[\"category\"].astype(str)\n",
        "    else: raise AssertionError(\"CSV에 cls(또는 category) 컬럼이 필요합니다.\")\n",
        "\n",
        "    has_fp    = \"filepath\" in df.columns\n",
        "    has_wpath = \"wav_path\" in df.columns\n",
        "    has_wfile = \"wav_file\" in df.columns\n",
        "\n",
        "    filepaths, floors, sources, intens, dists, reps = [], [], [], [], [], []\n",
        "    fixed, missing = 0, 0\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        category = str(row[\"category\"] if \"category\" in df.columns else row[\"cls\"]) if (\"category\" in df.columns or \"cls\" in df.columns) else \"\"\n",
        "        wav_file = str(row[\"wav_file\"]) if has_wfile and pd.notna(row[\"wav_file\"]) else \"\"\n",
        "        wav_path = str(row[\"wav_path\"]) if has_wpath and pd.notna(row[\"wav_path\"]) else \"\"\n",
        "\n",
        "        cand = None\n",
        "        if has_fp and pd.notna(row.get(\"filepath\", None)):\n",
        "            p = Path(str(row[\"filepath\"]))\n",
        "            if p.exists():\n",
        "                cand = str(p)\n",
        "            else:\n",
        "                cand = _try_candidates(split, category, wav_file, wav_path or p.as_posix(), data_root)\n",
        "                if cand: fixed += 1\n",
        "        else:\n",
        "            cand = _try_candidates(split, category, wav_file, wav_path, data_root)\n",
        "\n",
        "        if cand is None:\n",
        "            filepaths.append(\"__MISSING__\"); missing += 1\n",
        "            floors.append(np.nan); sources.append(None); intens.append(np.nan); dists.append(np.nan); reps.append(np.nan)\n",
        "            continue\n",
        "\n",
        "        filepaths.append(cand)\n",
        "        f,s,i,d,r = parse_meta_from_filename(Path(cand).name)\n",
        "        floors.append(f); sources.append(s); intens.append(i); dists.append(d); reps.append(r)\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"filepath\": filepaths,\n",
        "        \"db\": db,\n",
        "        \"cls\": cls,\n",
        "        \"floor\": floors,\n",
        "        \"source_from_name\": sources,\n",
        "        \"intensity\": intens,\n",
        "        \"distance\": dists,\n",
        "        \"repeat\": reps,\n",
        "    }).dropna(subset=[\"db\"])\n",
        "\n",
        "    if missing: print(f\"[경고] {split}: 자동 보정에도 불구하고 찾지 못한 파일 {missing}개\")\n",
        "    if fixed:   print(f\"[정보] {split}: 경로 자동 보정 성공 {fixed}개\")\n",
        "    out = out[(out[\"filepath\"] != \"__MISSING__\")].reset_index(drop=True)\n",
        "\n",
        "    # 파일명 소음원이 있으면 cls를 파일명 기준으로 교정\n",
        "    mask_fix = out[\"source_from_name\"].notna()\n",
        "    out.loc[mask_fix, \"cls\"] = out.loc[mask_fix, \"source_from_name\"].astype(str)\n",
        "\n",
        "    print(f\"[{split}] resolved rows = {len(out)} (원본 {len(df)})\")\n",
        "    if len(out): print(out.head())\n",
        "    return out\n",
        "\n",
        "# 표준화 & 보정\n",
        "train_src = normalize_split(train_src, \"train\", DATA_ROOT)\n",
        "val_src   = normalize_split(val_src,   \"val\",   DATA_ROOT)\n",
        "test_src  = normalize_split(test_src,  \"test\",  DATA_ROOT)\n",
        "assert len(train_src) and len(val_src) and len(test_src), \"빈 split이 있습니다.\"\n",
        "\n",
        "# ---------------- 특징 추출 (Log-Mel) ----------------\n",
        "def load_audio(fp):\n",
        "    y, sr = librosa.load(fp, sr=CFG[\"SR\"], mono=True)\n",
        "    T = int(CFG[\"SR\"]*CFG[\"DURATION\"])\n",
        "    if len(y) < T: y = np.pad(y, (0, T-len(y)))\n",
        "    else:          y = y[:T]\n",
        "    return y, sr\n",
        "\n",
        "def wav_to_logmel(fp):\n",
        "    y, sr = load_audio(fp)\n",
        "    S = librosa.feature.melspectrogram(\n",
        "        y=y, sr=sr,\n",
        "        n_mels=CFG[\"N_MELS\"], fmin=CFG[\"FMIN\"], fmax=CFG[\"FMAX\"],\n",
        "        n_fft=CFG[\"N_FFT\"], hop_length=CFG[\"HOP\"], power=2.0\n",
        "    )\n",
        "    X = librosa.power_to_db(S, ref=np.max)  # [n_mels, time]\n",
        "    mn, mx = X.min(), X.max()\n",
        "    X = (X - mn) / (mx - mn + 1e-8)         # [0,1]\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "def spec_key(fp):\n",
        "    s = f\"{fp}_LM_{CFG['SR']}_{CFG['DURATION']}_{CFG['N_FFT']}_{CFG['HOP']}_{CFG['N_MELS']}_{CFG['FMIN']}_{CFG['FMAX']}\"\n",
        "    return md5(s.encode()).hexdigest()\n",
        "\n",
        "CACHE_DRIVE = DATA_ROOT/\"logmel_cache_np\"\n",
        "CACHE_DRIVE.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "def ensure_cached(df, split):\n",
        "    from tqdm import tqdm\n",
        "    csv_path = DATA_ROOT/f\"{CSV_PREFIX}_{split}.csv\"\n",
        "    spaths = []\n",
        "    for fp in tqdm(df[\"filepath\"].tolist(), desc=f\"[cache] {split}\"):\n",
        "        npy = CACHE_DRIVE/f\"{spec_key(fp)}.npy\"\n",
        "        if not npy.exists(): np.save(npy, wav_to_logmel(fp))\n",
        "        spaths.append(str(npy))\n",
        "    out = df.copy()\n",
        "    out[\"spec_path\"] = spaths\n",
        "    for c in [\"floor\",\"intensity\",\"distance\",\"repeat\",\"db\"]:\n",
        "        if c in out.columns:\n",
        "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
        "    out.to_csv(csv_path, index=False)\n",
        "    return out\n",
        "\n",
        "train_df = ensure_cached(train_src, \"train\")\n",
        "val_df   = ensure_cached(val_src,   \"val\")\n",
        "test_df  = ensure_cached(test_src,  \"test\")\n",
        "\n",
        "# ---------------- EDA 플롯 (옵션) ----------------\n",
        "if CFG.get(\"PLOT_EDA\", True):\n",
        "    eda_dir = DATA_ROOT/\"eda\"\n",
        "    eda_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    def _bar_count(series, title, save):\n",
        "        cnt = series.value_counts().sort_index()\n",
        "        plt.figure()\n",
        "        plt.bar(cnt.index.astype(str), cnt.values)\n",
        "        plt.title(title); plt.xlabel(\"category\"); plt.ylabel(\"count\")\n",
        "        plt.tight_layout(); plt.savefig(eda_dir/save, dpi=150); plt.close()\n",
        "\n",
        "    def _hist(series, title, save, bins=20):\n",
        "        vals = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
        "        plt.figure()\n",
        "        plt.hist(vals.values, bins=bins)\n",
        "        plt.title(title); plt.xlabel(\"value\"); plt.ylabel(\"freq\")\n",
        "        plt.tight_layout(); plt.savefig(eda_dir/save, dpi=150); plt.close()\n",
        "\n",
        "    all_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
        "    _bar_count(all_df[\"floor\"].dropna().astype(int), \"Floor distribution\", \"count_floor.png\")\n",
        "    _bar_count(all_df[\"cls\"].astype(str), \"Source(class) distribution\", \"count_source.png\")\n",
        "    _hist(all_df[\"intensity\"], \"Intensity histogram\", \"hist_intensity.png\")\n",
        "    _hist(all_df[\"distance\"],  \"Distance histogram\",  \"hist_distance.png\")\n",
        "\n",
        "# ---------------- 요약 ----------------\n",
        "classes = sorted(pd.Series(train_df[\"cls\"].astype(str).unique()).tolist())\n",
        "meta = {\n",
        "    \"config\": CFG,\n",
        "    \"classes\": classes,\n",
        "    \"cache_dir\": str(CACHE_DRIVE),\n",
        "    \"csv_prefix\": CSV_PREFIX,\n",
        "    \"rows\": {\"train\": len(train_df), \"val\": len(val_df), \"test\": len(test_df)},\n",
        "    \"num_features\": [\"floor\",\"intensity\",\"distance\",\"repeat\"]\n",
        "}\n",
        "with open(DATA_ROOT/\"preprocess_meta.json\", \"w\") as f:\n",
        "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"\\n[완료] 전처리/캐시 생성\")\n",
        "print(\" - Cache dir :\", CACHE_DRIVE)\n",
        "print(\" - CSVs      :\", [DATA_ROOT/f'{CSV_PREFIX}_train.csv',\n",
        "                        DATA_ROOT/f'{CSV_PREFIX}_val.csv',\n",
        "                        DATA_ROOT/f'{CSV_PREFIX}_test.csv'])\n",
        "print(\" - Meta      :\", DATA_ROOT/'preprocess_meta.json')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuI0bVSomLdQ",
        "outputId": "02cb0d7a-5a55-4208-baaf-f466b7bee90d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[train] resolved rows = 2262 (원본 2262)\n",
            "                                            filepath         db   cls  floor  \\\n",
            "0  /content/drive/MyDrive/kaggle_noise/train/data...  60.200001  book      2   \n",
            "1  /content/drive/MyDrive/kaggle_noise/train/data...  61.049999  book      2   \n",
            "2  /content/drive/MyDrive/kaggle_noise/train/data...  67.110001  book      2   \n",
            "3  /content/drive/MyDrive/kaggle_noise/train/data...  53.680000  book      2   \n",
            "4  /content/drive/MyDrive/kaggle_noise/train/data...  66.139999  book      2   \n",
            "\n",
            "  source_from_name  intensity  distance  repeat  \n",
            "0             book        3.0       1.0      42  \n",
            "1             book        2.0       5.0     138  \n",
            "2             book        3.0       1.0     175  \n",
            "3             book        3.0       5.0      64  \n",
            "4             book        2.0       1.0      22  \n",
            "[val] resolved rows = 646 (원본 646)\n",
            "                                            filepath         db   cls  floor  \\\n",
            "0  /content/drive/MyDrive/kaggle_noise/val/data/b...  66.139999  book      2   \n",
            "1  /content/drive/MyDrive/kaggle_noise/val/data/b...  63.750000  book      2   \n",
            "2  /content/drive/MyDrive/kaggle_noise/val/data/b...  60.990002  book      2   \n",
            "3  /content/drive/MyDrive/kaggle_noise/val/data/b...  67.790001  book      2   \n",
            "4  /content/drive/MyDrive/kaggle_noise/val/data/b...  63.939999  book      2   \n",
            "\n",
            "  source_from_name  intensity  distance  repeat  \n",
            "0             book        2.0       1.0      24  \n",
            "1             book        3.0       5.0     125  \n",
            "2             book        2.0       3.0     156  \n",
            "3             book        2.0       3.0      20  \n",
            "4             book        2.0       3.0     220  \n",
            "[test] resolved rows = 327 (원본 327)\n",
            "                                            filepath         db   cls  floor  \\\n",
            "0  /content/drive/MyDrive/kaggle_noise/test/data/...  63.099998  book      2   \n",
            "1  /content/drive/MyDrive/kaggle_noise/test/data/...  72.510002  book      2   \n",
            "2  /content/drive/MyDrive/kaggle_noise/test/data/...  61.099998  book      2   \n",
            "3  /content/drive/MyDrive/kaggle_noise/test/data/...  64.330002  book      2   \n",
            "4  /content/drive/MyDrive/kaggle_noise/test/data/...  57.520000  book      2   \n",
            "\n",
            "  source_from_name  intensity  distance  repeat  \n",
            "0             book        3.0       5.0     128  \n",
            "1             book        1.0       3.0     173  \n",
            "2             book        1.0       3.0     230  \n",
            "3             book        2.0       3.0       2  \n",
            "4             book        3.0       1.0     234  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[cache] train: 100%|██████████| 2262/2262 [00:04<00:00, 454.63it/s]\n",
            "[cache] val: 100%|██████████| 646/646 [00:00<00:00, 4791.15it/s]\n",
            "[cache] test: 100%|██████████| 327/327 [00:00<00:00, 4913.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[완료] 전처리/캐시 생성\n",
            " - Cache dir : /content/drive/MyDrive/kaggle_noise/logmel_cache_np\n",
            " - CSVs      : [PosixPath('/content/drive/MyDrive/kaggle_noise/ast_split_with_spec_train.csv'), PosixPath('/content/drive/MyDrive/kaggle_noise/ast_split_with_spec_val.csv'), PosixPath('/content/drive/MyDrive/kaggle_noise/ast_split_with_spec_test.csv')]\n",
            " - Meta      : /content/drive/MyDrive/kaggle_noise/preprocess_meta.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, math, random, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "try:\n",
        "    import timm\n",
        "except:\n",
        "    raise SystemExit(\"timm가 필요합니다. 먼저 `pip install timm` 실행하세요.\")\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "CFG = {\n",
        "    \"DATA_ROOT\": \"/content/drive/MyDrive/kaggle_noise\",  # ★ 전처리와 동일\n",
        "    \"CSV_PREFIX\": \"ast_split_with_spec\",\n",
        "    \"SEED\": 42,\n",
        "    \"BATCH\": 32,\n",
        "    \"EPOCHS\": 60,\n",
        "    \"LR\": 2e-4,\n",
        "    \"WEIGHT_DECAY\": 1e-4,\n",
        "    \"WARMUP_EPOCHS\": 4,\n",
        "    \"HUBER_DELTA\": 1.5,\n",
        "    \"CLS_SMOOTH\": 0.05,\n",
        "    \"EMA_DECAY\": 0.999,\n",
        "    \"EARLY_STOP\": 10,\n",
        "    \"MIXED_PREC\": True,\n",
        "    # SpecAug (기본 off)\n",
        "    \"SPEC_FREQ_MASKS\": 2, \"SPEC_FREQ_MAXW\": 16,\n",
        "    \"SPEC_TIME_MASKS\": 2, \"SPEC_TIME_MAXW\": 24,\n",
        "    \"SPEC_AUG_PROB\": 0.0,\n",
        "    # 로더\n",
        "    \"NUM_WORKERS\": 2, \"PIN_MEMORY\": True, \"PERSISTENT\": True, \"PREFETCH\": 2,\n",
        "    # 스코어\n",
        "    \"SCORE_ALPHA\": 1.0,\n",
        "    \"SAVE_DIR\": \"convnext_tiny_multi_ckpt\",\n",
        "    # 시간축 시프트 (train만)\n",
        "    \"TIME_SHIFT_MAX\": 24,\n",
        "    # 검증/테스트 TTA\n",
        "    \"TTA_N\": 1, \"TTA_MAX_SHIFT\": 0,\n",
        "    # 메타 피처\n",
        "    \"USE_META\": True,\n",
        "    \"META_COLS\": [\"floor\", \"intensity\", \"distance\", \"repeat\"],\n",
        "    \"META_EMB_DIM\": 64,\n",
        "}\n",
        "\n",
        "RESIZE_HW = (224, 224)\n",
        "\n",
        "# ---------------- Utils ----------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    if torch.__version__ >= \"2.0\":\n",
        "        try: torch.set_float32_matmul_precision(\"high\")\n",
        "        except: pass\n",
        "\n",
        "def label_smoothing_ce(logits, target, smoothing=0.0):\n",
        "    if smoothing <= 0: return F.cross_entropy(logits, target)\n",
        "    n = logits.size(-1)\n",
        "    with torch.no_grad():\n",
        "        true = torch.zeros_like(logits).fill_(smoothing / (n - 1))\n",
        "        true.scatter_(1, target.unsqueeze(1), 1.0 - smoothing)\n",
        "    logp = F.log_softmax(logits, dim=1)\n",
        "    return -(true * logp).sum(dim=1).mean()\n",
        "\n",
        "def spec_augment(x, p=0.7, f_masks=2, f_w=16, t_masks=2, t_w=24):\n",
        "    if p <= 0 or random.random() > p: return x\n",
        "    B, C, Fm, Tm = x.shape\n",
        "    x = x.clone()\n",
        "    for b in range(B):\n",
        "        for _ in range(f_masks):\n",
        "            w = random.randint(0, max(0, f_w))\n",
        "            if w == 0: continue\n",
        "            f0 = random.randint(0, max(0, Fm - w))\n",
        "            x[b, :, f0:f0+w, :] = 0.0\n",
        "        for _ in range(t_masks):\n",
        "            w = random.randint(0, max(0, t_w))\n",
        "            if w == 0: continue\n",
        "            t0 = random.randint(0, max(0, Tm - w))\n",
        "            x[b, :, :, t0:t0+w] = 0.0\n",
        "    return x\n",
        "\n",
        "def per_image_standardize(x: torch.Tensor) -> torch.Tensor:\n",
        "    if x.dim() == 3:\n",
        "        mean = x.mean(dim=(1, 2), keepdim=True)\n",
        "        std  = x.std(dim=(1, 2), keepdim=True).clamp_min(1e-5)\n",
        "        return (x - mean) / std\n",
        "    elif x.dim() == 4:\n",
        "        mean = x.mean(dim=(2, 3), keepdim=True)\n",
        "        std  = x.std(dim=(2, 3), keepdim=True).clamp_min(1e-5)\n",
        "        return (x - mean) / std\n",
        "    else:\n",
        "        raise ValueError(f\"per_image_standardize: unexpected shape {tuple(x.shape)}\")\n",
        "\n",
        "def time_shift(x, max_shift):\n",
        "    if max_shift <= 0: return x\n",
        "    s = random.randint(-max_shift, max_shift)\n",
        "    return torch.roll(x, shifts=s, dims=3)\n",
        "\n",
        "# ---------------- EMA ----------------\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {n: p.detach().clone() for n,p in model.named_parameters() if p.requires_grad}\n",
        "        self.backup = {}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for n, p in model.named_parameters():\n",
        "            if not p.requires_grad: continue\n",
        "            self.shadow[n] = self.decay*self.shadow[n] + (1.0-self.decay)*p.detach()\n",
        "\n",
        "    def apply_shadow(self, model):\n",
        "        self.backup = {}\n",
        "        for n, p in model.named_parameters():\n",
        "            if not p.requires_grad: continue\n",
        "            self.backup[n] = p.detach().clone()\n",
        "            p.data.copy_(self.shadow[n].data)\n",
        "\n",
        "    def restore(self, model):\n",
        "        for n, p in model.named_parameters():\n",
        "            if not p.requires_grad: continue\n",
        "            p.data.copy_(self.backup[n].data)\n",
        "        self.backup = {}\n",
        "\n",
        "# ---------------- Dataset ----------------\n",
        "class LogMelDataset(Dataset):\n",
        "    def __init__(self, csv_path, le=None, meta_cols=None, scaler=None, fit_scaler=False):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        assert {\"spec_path\",\"db\",\"cls\"}.issubset(self.df.columns)\n",
        "        self.paths = self.df[\"spec_path\"].tolist()\n",
        "        self.db = self.df[\"db\"].astype(np.float32).values\n",
        "        self.cls_raw = self.df[\"cls\"].astype(str).values\n",
        "\n",
        "        if le is None:\n",
        "            self.le = LabelEncoder()\n",
        "            self.cls = self.le.fit_transform(self.cls_raw)\n",
        "        else:\n",
        "            self.le = le\n",
        "            self.cls = self.le.transform(self.cls_raw)\n",
        "\n",
        "        self.meta_cols = meta_cols or []\n",
        "        self.has_meta = len(self.meta_cols) > 0 and set(self.meta_cols).issubset(self.df.columns)\n",
        "        self.scaler = scaler\n",
        "        if self.has_meta:\n",
        "            Xm = self.df[self.meta_cols].copy()\n",
        "            for c in self.meta_cols:\n",
        "                Xm[c] = pd.to_numeric(Xm[c], errors=\"coerce\")\n",
        "            Xm = Xm.fillna(Xm.mean())\n",
        "            if fit_scaler:\n",
        "                self.scaler = StandardScaler()\n",
        "                self.scaler.fit(Xm.values.astype(np.float32))\n",
        "            self.meta_np = self.scaler.transform(Xm.values.astype(np.float32)) if self.scaler is not None else Xm.values.astype(np.float32)\n",
        "        else:\n",
        "            self.meta_np = None\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        X = np.load(self.paths[i], mmap_mode=\"r\")\n",
        "        X = np.array(X, dtype=np.float32, copy=True)  # [F,T]\n",
        "        x = torch.from_numpy(X).unsqueeze(0)          # [1,F,T]\n",
        "        x = F.interpolate(x.unsqueeze(0), size=RESIZE_HW,\n",
        "                          mode=\"bilinear\", align_corners=False).squeeze(0)  # [1,224,224]\n",
        "        x = per_image_standardize(x)\n",
        "        y_reg = torch.tensor(self.db[i], dtype=torch.float32)\n",
        "        y_cls = torch.tensor(self.cls[i], dtype=torch.long)\n",
        "\n",
        "        if self.has_meta:\n",
        "            m = torch.tensor(self.meta_np[i], dtype=torch.float32)  # [M]\n",
        "            return x, m, y_reg, y_cls\n",
        "        else:\n",
        "            return x, None, y_reg, y_cls\n",
        "\n",
        "# ---------------- Model ----------------\n",
        "class ConvNeXtMultiTaskMeta(nn.Module):\n",
        "    def __init__(self, num_classes, meta_dim=0, meta_emb=64):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\"convnext_tiny\", pretrained=True, in_chans=1, num_classes=0)\n",
        "        feat_dim = self.backbone.num_features  # 768\n",
        "        self.use_meta = meta_dim > 0\n",
        "        if self.use_meta:\n",
        "            self.meta_mlp = nn.Sequential(\n",
        "                nn.Linear(meta_dim, meta_emb), nn.GELU(),\n",
        "                nn.LayerNorm(meta_emb),\n",
        "            )\n",
        "            head_in = feat_dim + meta_emb\n",
        "        else:\n",
        "            self.meta_mlp = None\n",
        "            head_in = feat_dim\n",
        "\n",
        "        self.head_reg = nn.Sequential(\n",
        "            nn.LayerNorm(head_in), nn.Linear(head_in, head_in//2), nn.GELU(), nn.Dropout(0.1),\n",
        "            nn.Linear(head_in//2, 1)\n",
        "        )\n",
        "        self.head_cls = nn.Sequential(\n",
        "            nn.LayerNorm(head_in), nn.Linear(head_in, head_in//2), nn.GELU(), nn.Dropout(0.1),\n",
        "            nn.Linear(head_in//2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, m=None):\n",
        "        feat = self.backbone(x)  # [B,C]\n",
        "        if self.use_meta and m is not None:\n",
        "            m_emb = self.meta_mlp(m)  # [B,meta_emb]\n",
        "            feat = torch.cat([feat, m_emb], dim=1)\n",
        "        y_reg = self.head_reg(feat).squeeze(1)\n",
        "        y_cls = self.head_cls(feat)\n",
        "        return y_reg, y_cls\n",
        "\n",
        "# ---------------- Sched / Loader ----------------\n",
        "def mae(x, y): return torch.mean(torch.abs(x - y))\n",
        "\n",
        "def cosine_with_warmup(optimizer, warmup_epochs, total_epochs):\n",
        "    def lr_lambda(current_epoch):\n",
        "        if current_epoch < warmup_epochs:\n",
        "            return float(current_epoch + 1) / float(max(1, warmup_epochs))\n",
        "        progress = (current_epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "def make_loader(ds, bs, shuffle):\n",
        "    return DataLoader(\n",
        "        ds, batch_size=bs, shuffle=shuffle,\n",
        "        num_workers=CFG[\"NUM_WORKERS\"], pin_memory=CFG[\"PIN_MEMORY\"],\n",
        "        persistent_workers=CFG[\"PERSISTENT\"], prefetch_factor=CFG[\"PREFETCH\"]\n",
        "    )\n",
        "\n",
        "# ---------------- TTA ----------------\n",
        "def tta_time_roll(x: torch.Tensor, n: int, max_shift: int):\n",
        "    if n <= 1 or max_shift <= 0: return [x]\n",
        "    outs = [x]\n",
        "    for _ in range(n-1):\n",
        "        s = random.randint(-max_shift, max_shift)\n",
        "        outs.append(torch.roll(x, shifts=s, dims=3))\n",
        "    return outs\n",
        "\n",
        "# ---------------- Plot helpers ----------------\n",
        "def plot_curve(xs, ys, title, xlabel, ylabel, save_path):\n",
        "    plt.figure()\n",
        "    plt.plot(xs, ys)\n",
        "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
        "    plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    plt.tight_layout(); plt.savefig(save_path, dpi=150); plt.close()\n",
        "\n",
        "def plot_mae_dual(hist_df, save_path):\n",
        "    \"\"\"요청 그래프: train/val MAE 함께 그리고, best val에 세로 점선.\"\"\"\n",
        "    xs = hist_df[\"epoch\"].values\n",
        "    y_tr = hist_df[\"tr_reg_mae\"].values\n",
        "    y_va = hist_df[\"va_mae\"].values\n",
        "    best_i = int(np.nanargmin(y_va))\n",
        "    best_epoch = int(xs[best_i])\n",
        "    best_val = float(y_va[best_i])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(xs, y_tr, label=\"train\")\n",
        "    plt.plot(xs, y_va, label=\"val\")\n",
        "    plt.axvline(best_epoch, linestyle=\"--\")  # 세로 점선\n",
        "    plt.title(f\"MAE (best@{best_epoch}, val={best_val:.3f})\")\n",
        "    plt.xlabel(\"epoch\"); plt.ylabel(\"MAE(dB)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    plt.tight_layout(); plt.savefig(save_path, dpi=150); plt.close()\n",
        "\n",
        "def plot_scatter(x, y, title, xlabel, ylabel, save_path, refline=True):\n",
        "    plt.figure()\n",
        "    plt.scatter(x, y, s=10, alpha=0.6)\n",
        "    if refline:\n",
        "        lo = min(np.min(x), np.min(y)); hi = max(np.max(x), np.max(y))\n",
        "        plt.plot([lo, hi], [lo, hi])\n",
        "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
        "    plt.tight_layout(); plt.savefig(save_path, dpi=150); plt.close()\n",
        "\n",
        "def plot_hist(vals, title, xlabel, save_path, bins=30):\n",
        "    plt.figure()\n",
        "    plt.hist(vals, bins=bins)\n",
        "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(\"freq\")\n",
        "    plt.tight_layout(); plt.savefig(save_path, dpi=150); plt.close()\n",
        "\n",
        "def plot_confmat(y_true, y_pred, label_map, save_path, normalize=True):\n",
        "    labels = [label_map[i] for i in range(len(label_map))]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n",
        "    if normalize:\n",
        "        cm = cm.astype(np.float32)\n",
        "        cm = cm / (cm.sum(axis=1, keepdims=True) + 1e-9)\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.title(\"Confusion Matrix\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "    plt.xticks(ticks=np.arange(len(labels)), labels=labels, rotation=45, ha=\"right\")\n",
        "    plt.yticks(ticks=np.arange(len(labels)), labels=labels)\n",
        "    plt.colorbar()\n",
        "    plt.tight_layout(); plt.savefig(save_path, dpi=150); plt.close()\n",
        "\n",
        "# ---------------- Train/Eval ----------------\n",
        "def train_one_epoch(model, loader, optim, amp_scaler, device, ema=None):\n",
        "    model.train()\n",
        "    tot, loss_sum, mae_sum, cls_sum = 0, 0.0, 0.0, 0.0\n",
        "    for batch in loader:\n",
        "        if CFG[\"USE_META\"]:\n",
        "            x, m, y_reg, y_cls = batch\n",
        "            m = m.to(device, non_blocking=True)\n",
        "        else:\n",
        "            x, _, y_reg, y_cls = batch\n",
        "            m = None\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y_reg = y_reg.to(device, non_blocking=True)\n",
        "        y_cls = y_cls.to(device, non_blocking=True)\n",
        "\n",
        "        x = time_shift(x, CFG[\"TIME_SHIFT_MAX\"])\n",
        "        if CFG[\"SPEC_AUG_PROB\"] > 0:\n",
        "            x = spec_augment(x, p=CFG[\"SPEC_AUG_PROB\"],\n",
        "                             f_masks=CFG[\"SPEC_FREQ_MASKS\"], f_w=CFG[\"SPEC_FREQ_MAXW\"],\n",
        "                             t_masks=CFG[\"SPEC_TIME_MASKS\"], t_w=CFG[\"SPEC_TIME_MAXW\"])\n",
        "\n",
        "        optim.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast(device_type='cuda', enabled=CFG[\"MIXED_PREC\"] and torch.cuda.is_available()):\n",
        "            pr_reg, pr_cls = model(x, m)\n",
        "            loss_reg = F.huber_loss(pr_reg, y_reg, delta=CFG[\"HUBER_DELTA\"])\n",
        "            loss_cls = label_smoothing_ce(pr_cls, y_cls, smoothing=CFG[\"CLS_SMOOTH\"])\n",
        "            loss = loss_reg + loss_cls\n",
        "\n",
        "        if amp_scaler is not None:\n",
        "            amp_scaler.scale(loss).backward()\n",
        "            amp_scaler.unscale_(optim)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            amp_scaler.step(optim); amp_scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optim.step()\n",
        "\n",
        "        if ema is not None: ema.update(model)\n",
        "\n",
        "        bsz = x.size(0); tot += bsz\n",
        "        loss_sum += loss.item() * bsz\n",
        "        cls_sum  += loss_cls.item() * bsz\n",
        "        with torch.no_grad(): mae_sum += mae(pr_reg, y_reg).item() * bsz\n",
        "\n",
        "    return {\"loss\": loss_sum/tot, \"reg_mae\": mae_sum/tot, \"cls_loss\": cls_sum/tot}\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, le, use_ema=None, collect=False):\n",
        "    model.eval()\n",
        "    if use_ema is not None: use_ema.apply_shadow(model)\n",
        "\n",
        "    tot = 0\n",
        "    loss_sum = 0.0; mae_sum = 0.0; cls_sum = 0.0\n",
        "    y_true = []; y_pred = []\n",
        "    y_reg_true = []; y_reg_pred = []\n",
        "\n",
        "    for batch in loader:\n",
        "        if CFG[\"USE_META\"]:\n",
        "            x, m, y_reg, y_cls = batch\n",
        "            m = m.to(device, non_blocking=True)\n",
        "        else:\n",
        "            x, _, y_reg, y_cls = batch\n",
        "            m = None\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y_reg = y_reg.to(device, non_blocking=True)\n",
        "        y_cls = y_cls.to(device, non_blocking=True)\n",
        "\n",
        "        x_list = tta_time_roll(x, CFG[\"TTA_N\"], CFG[\"TTA_MAX_SHIFT\"])\n",
        "        pr_reg_list = []; pr_cls_list = []\n",
        "        for xt in x_list:\n",
        "            pr_reg_t, pr_cls_t = model(xt, m)\n",
        "            pr_reg_list.append(pr_reg_t); pr_cls_list.append(pr_cls_t)\n",
        "        pr_reg = torch.stack(pr_reg_list, dim=0).mean(0)\n",
        "        pr_cls = torch.stack(pr_cls_list, dim=0).mean(0)\n",
        "\n",
        "        loss_reg = F.huber_loss(pr_reg, y_reg, delta=CFG[\"HUBER_DELTA\"])\n",
        "        loss_cls = F.cross_entropy(pr_cls, y_cls)\n",
        "        loss = loss_reg + loss_cls\n",
        "\n",
        "        bsz = x.size(0); tot += bsz\n",
        "        loss_sum += loss.item() * bsz\n",
        "        cls_sum  += loss_cls.item() * bsz\n",
        "        mae_sum  += torch.mean(torch.abs(pr_reg - y_reg)).item() * bsz\n",
        "\n",
        "        y_true.append(y_cls.cpu().numpy())\n",
        "        y_pred.append(pr_cls.argmax(1).cpu().numpy())\n",
        "        y_reg_true.append(y_reg.cpu().numpy())\n",
        "        y_reg_pred.append(pr_reg.cpu().numpy())\n",
        "\n",
        "    if use_ema is not None: use_ema.restore(model)\n",
        "\n",
        "    y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n",
        "    y_reg_true = np.concatenate(y_reg_true); y_reg_pred = np.concatenate(y_reg_pred)\n",
        "\n",
        "    f1_macro = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
        "    acc = float((y_true == y_pred).mean())\n",
        "    diff = y_reg_true - y_reg_pred\n",
        "    mae_val = float(np.mean(np.abs(diff)))\n",
        "    rmse = float(np.sqrt(np.mean(diff**2)))\n",
        "\n",
        "    out = {\n",
        "        \"loss\": loss_sum / tot, \"reg_mae\": mae_sum / tot, \"cls_loss\": cls_sum / tot,\n",
        "        \"f1_macro\": f1_macro, \"acc\": acc, \"mae\": mae_val, \"rmse\": rmse,\n",
        "        \"label_map\": dict(enumerate(le.classes_.tolist()))\n",
        "    }\n",
        "    if collect:\n",
        "        out.update({\"y_true\": y_true, \"y_pred\": y_pred,\n",
        "                    \"y_reg_true\": y_reg_true, \"y_reg_pred\": y_reg_pred})\n",
        "    return out\n",
        "\n",
        "def composite_score(mae_val, f1_macro, alpha=1.0):\n",
        "    return mae_val - alpha * f1_macro\n",
        "\n",
        "# ---------------- Main ----------------\n",
        "def main():\n",
        "    set_seed(CFG[\"SEED\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    data_root = Path(CFG[\"DATA_ROOT\"])\n",
        "    save_dir = Path(CFG[\"SAVE_DIR\"]); save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_csv = data_root / f\"{CFG['CSV_PREFIX']}_train.csv\"\n",
        "    val_csv   = data_root / f\"{CFG['CSV_PREFIX']}_val.csv\"\n",
        "    test_csv  = data_root / f\"{CFG['CSV_PREFIX']}_test.csv\"\n",
        "    assert train_csv.exists() and val_csv.exists() and test_csv.exists(), \"전처리 CSV가 없습니다. preprocess 먼저!\"\n",
        "\n",
        "    tmp = pd.read_csv(train_csv)\n",
        "    le = LabelEncoder().fit(tmp[\"cls\"].astype(str).values)\n",
        "    num_classes = len(le.classes_)\n",
        "    print(\"[classes]\", le.classes_)\n",
        "\n",
        "    # 메타 스케일러 적합\n",
        "    meta_cols = CFG[\"META_COLS\"] if CFG[\"USE_META\"] else []\n",
        "    meta_scaler = None\n",
        "    if CFG[\"USE_META\"]:\n",
        "        Xm = tmp[meta_cols].copy()\n",
        "        for c in meta_cols:\n",
        "            Xm[c] = pd.to_numeric(Xm[c], errors=\"coerce\")\n",
        "        meta_scaler = StandardScaler().fit(Xm.values.astype(np.float32))\n",
        "\n",
        "    # 데이터셋/로더\n",
        "    train_ds = LogMelDataset(train_csv, le=le, meta_cols=meta_cols, scaler=meta_scaler, fit_scaler=False)\n",
        "    val_ds   = LogMelDataset(val_csv,   le=le, meta_cols=meta_cols, scaler=meta_scaler, fit_scaler=False)\n",
        "    test_ds  = LogMelDataset(test_csv,  le=le, meta_cols=meta_cols, scaler=meta_scaler, fit_scaler=False)\n",
        "\n",
        "    train_loader = make_loader(train_ds, CFG[\"BATCH\"], True)\n",
        "    val_loader   = make_loader(val_ds,   CFG[\"BATCH\"], False)\n",
        "    test_loader  = make_loader(test_ds,  CFG[\"BATCH\"], False)\n",
        "\n",
        "    meta_dim = len(meta_cols) if CFG[\"USE_META\"] else 0\n",
        "    model = ConvNeXtMultiTaskMeta(num_classes=num_classes, meta_dim=meta_dim, meta_emb=CFG[\"META_EMB_DIM\"]).to(device)\n",
        "\n",
        "    # 회귀 바이어스 = 학습셋 평균 dB\n",
        "    train_mean = float(tmp[\"db\"].astype(float).mean())\n",
        "    with torch.no_grad():\n",
        "        model.head_reg[-1].bias.data.fill_(train_mean)\n",
        "\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=CFG[\"LR\"], weight_decay=CFG[\"WEIGHT_DECAY\"])\n",
        "    amp_scaler = torch.amp.GradScaler('cuda', enabled=CFG[\"MIXED_PREC\"] and torch.cuda.is_available())\n",
        "    sched = cosine_with_warmup(optim, CFG[\"WARMUP_EPOCHS\"], CFG[\"EPOCHS\"])\n",
        "    ema = EMA(model, decay=CFG[\"EMA_DECAY\"])\n",
        "\n",
        "    best_score = 1e9\n",
        "    bad = 0\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(1, CFG[\"EPOCHS\"]+1):\n",
        "        t0 = time.time()\n",
        "        tr = train_one_epoch(model, train_loader, optim, amp_scaler, device, ema=ema)\n",
        "        va = evaluate(model, val_loader, device, le, use_ema=ema, collect=False)\n",
        "        sched.step()\n",
        "\n",
        "        score = composite_score(va[\"mae\"], va[\"f1_macro\"], alpha=CFG[\"SCORE_ALPHA\"])\n",
        "        history.append({\"epoch\": epoch, **{f\"tr_{k}\": v for k,v in tr.items()},\n",
        "                        **{f\"va_{k}\": v for k,v in va.items()}, \"score\": float(score)})\n",
        "\n",
        "        print(\n",
        "            f\"[{epoch:03d}] \"\n",
        "            f\"tr_loss={tr['loss']:.4f} tr_mae={tr['reg_mae']:.3f} \"\n",
        "            f\"| va_mae={va['mae']:.3f} va_rmse={va['rmse']:.3f} \"\n",
        "            f\"| va_f1M={va['f1_macro']:.3f} va_acc={va['acc']:.3f} \"\n",
        "            f\"| score={score:.3f} | time={time.time()-t0:.1f}s\"\n",
        "        )\n",
        "\n",
        "        if score < best_score:\n",
        "            best_score = score; bad = 0\n",
        "            ema.apply_shadow(model)\n",
        "            torch.save(model.state_dict(), save_dir/\"best_convnext_tiny_multi_meta.pth\")\n",
        "            ema.restore(model)\n",
        "            with open(save_dir/\"label_map.json\", \"w\") as f:\n",
        "                json.dump({i:c for i,c in enumerate(le.classes_.tolist())}, f, ensure_ascii=False, indent=2)\n",
        "            with open(save_dir/\"train_history.json\", \"w\") as f:\n",
        "                json.dump(history, f, ensure_ascii=False, indent=2)\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= CFG[\"EARLY_STOP\"]:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # ----- Test (+ 그래프 저장용 상세 예측 수집) -----\n",
        "    model.load_state_dict(torch.load(save_dir/\"best_convnext_tiny_multi_meta.pth\", map_location=device))\n",
        "    te = evaluate(model, test_loader, device, le, use_ema=None, collect=True)\n",
        "    print(f\"\\n[Test] mae={te['mae']:.3f} rmse={te['rmse']:.3f} | f1M={te['f1_macro']:.3f} acc={te['acc']:.3f}\")\n",
        "\n",
        "    # ----- 기록/그래프 저장 -----\n",
        "    hist_df = pd.DataFrame(history)\n",
        "    hist_df.to_csv(save_dir/\"history.csv\", index=False)\n",
        "\n",
        "    # (요청) 이중 MAE 그래프 + 최적 에폭 점선\n",
        "    plot_mae_dual(hist_df, save_dir/\"curve_mae_dual.png\")\n",
        "\n",
        "    # 보조 곡선\n",
        "    plot_curve(hist_df[\"epoch\"], hist_df[\"tr_loss\"], \"Train Total Loss\", \"epoch\", \"loss\", save_dir/\"curve_losses_train.png\")\n",
        "    plot_curve(hist_df[\"epoch\"], hist_df[\"va_loss\"], \"Val Total Loss\", \"epoch\", \"loss\", save_dir/\"curve_losses.png\")\n",
        "    plot_curve(hist_df[\"epoch\"], hist_df[\"va_f1_macro\"], \"Val Macro-F1\", \"epoch\", \"F1\", save_dir/\"curve_f1.png\")\n",
        "    plot_curve(hist_df[\"epoch\"], hist_df[\"va_acc\"], \"Val Accuracy\", \"epoch\", \"Acc\", save_dir/\"curve_acc.png\")\n",
        "\n",
        "    # 회귀 시각화\n",
        "    plot_scatter(te[\"y_reg_true\"], te[\"y_reg_pred\"], \"Test dB: Pred vs True\", \"True dB\", \"Pred dB\", save_dir/\"test_pred_vs_true.png\")\n",
        "    plot_hist(te[\"y_reg_true\"] - te[\"y_reg_pred\"], \"Test Residuals\", \"True - Pred (dB)\", save_dir/\"test_residual_hist.png\")\n",
        "\n",
        "    # 분류 시각화\n",
        "    plot_confmat(te[\"y_true\"], te[\"y_pred\"], te[\"label_map\"], save_dir/\"test_cm.png\", normalize=True)\n",
        "\n",
        "    # ----- 메트릭/예측 저장 (배열 직렬화 주의) -----\n",
        "    np.savez(save_dir/\"test_arrays.npz\",\n",
        "             y_true=te[\"y_true\"], y_pred=te[\"y_pred\"],\n",
        "             y_reg_true=te[\"y_reg_true\"], y_reg_pred=te[\"y_reg_pred\"])\n",
        "\n",
        "    id2label = {int(k): v for k, v in te[\"label_map\"].items()}\n",
        "    pd.DataFrame({\"true_db\": te[\"y_reg_true\"], \"pred_db\": te[\"y_reg_pred\"]}).to_csv(save_dir/\"test_regression_preds.csv\", index=False)\n",
        "    cls_df = pd.DataFrame({\"true_id\": te[\"y_true\"].astype(int), \"pred_id\": te[\"y_pred\"].astype(int)})\n",
        "    cls_df[\"true_label\"] = cls_df[\"true_id\"].map(id2label)\n",
        "    cls_df[\"pred_label\"] = cls_df[\"pred_id\"].map(id2label)\n",
        "    cls_df.to_csv(save_dir/\"test_classification_preds.csv\", index=False)\n",
        "\n",
        "    te_save = {\"mae\": te[\"mae\"], \"rmse\": te[\"rmse\"], \"f1_macro\": te[\"f1_macro\"], \"acc\": te[\"acc\"],\n",
        "               \"label_map\": {int(k): v for k, v in te[\"label_map\"].items()}}\n",
        "    with open(save_dir/\"test_metrics.json\", \"w\") as f:\n",
        "        json.dump(te_save, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\n[완료] 모델/기록/그래프 저장:\", save_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4e2ed6530ded4ef78398b8358bc477ef",
            "4eb1867b557e459a850eebdcfb566e64",
            "b1561d7757e24c2f9f16458375ed38bb",
            "dcfd5e2d148b4f4aaafbde6eae156992",
            "3d717e3048a3444da96483d40e0ae1e4",
            "683133588445473f8586d6ac72de56eb",
            "e9081841309f4398bc7f657dea16eb48",
            "6b7871608f654efa981faf7a24aee05f",
            "8a35e3841aa041aeaa58ff720eb65f9d",
            "65ebb1fc5cbe45699e0d5d2fbe250a05",
            "f04d2e75faa8481f91007bc5ab50d320"
          ]
        },
        "id": "MpTpggv4m7rv",
        "outputId": "98cc4ace-39cb-4aad-dca3-2e79d9f06d71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[classes] ['book' 'chair' 'desk' 'hammer' 'lecturestand']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e2ed6530ded4ef78398b8358bc477ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[001] tr_loss=8.7104 tr_mae=5.874 | va_mae=7.932 va_rmse=9.817 | va_f1M=0.159 va_acc=0.217 | score=7.773 | time=363.0s\n",
            "[002] tr_loss=5.6356 tr_mae=4.100 | va_mae=7.785 va_rmse=9.666 | va_f1M=0.421 va_acc=0.457 | score=7.364 | time=23.3s\n",
            "[003] tr_loss=4.8590 tr_mae=3.636 | va_mae=7.501 va_rmse=9.361 | va_f1M=0.781 va_acc=0.779 | score=6.721 | time=23.8s\n",
            "[004] tr_loss=5.0232 tr_mae=3.723 | va_mae=7.043 va_rmse=8.868 | va_f1M=0.893 va_acc=0.887 | score=6.150 | time=22.5s\n",
            "[005] tr_loss=5.4647 tr_mae=4.003 | va_mae=6.496 va_rmse=8.283 | va_f1M=0.953 va_acc=0.949 | score=5.544 | time=23.4s\n",
            "[006] tr_loss=4.7104 tr_mae=3.494 | va_mae=5.917 va_rmse=7.654 | va_f1M=0.982 va_acc=0.981 | score=4.934 | time=23.5s\n",
            "[007] tr_loss=4.6360 tr_mae=3.486 | va_mae=5.307 va_rmse=7.000 | va_f1M=0.989 va_acc=0.989 | score=4.317 | time=23.7s\n",
            "[008] tr_loss=4.4383 tr_mae=3.344 | va_mae=4.800 va_rmse=6.466 | va_f1M=0.991 va_acc=0.991 | score=3.808 | time=22.7s\n",
            "[009] tr_loss=3.8751 tr_mae=2.993 | va_mae=4.446 va_rmse=6.073 | va_f1M=0.993 va_acc=0.992 | score=3.454 | time=23.5s\n",
            "[010] tr_loss=3.2798 tr_mae=2.598 | va_mae=4.167 va_rmse=5.736 | va_f1M=0.993 va_acc=0.992 | score=3.174 | time=24.0s\n",
            "[011] tr_loss=3.3825 tr_mae=2.677 | va_mae=3.952 va_rmse=5.471 | va_f1M=0.993 va_acc=0.992 | score=2.960 | time=24.0s\n",
            "[012] tr_loss=2.9648 tr_mae=2.403 | va_mae=3.774 va_rmse=5.239 | va_f1M=0.993 va_acc=0.992 | score=2.781 | time=23.3s\n",
            "[013] tr_loss=2.7963 tr_mae=2.286 | va_mae=3.603 va_rmse=5.016 | va_f1M=0.996 va_acc=0.995 | score=2.607 | time=23.4s\n",
            "[014] tr_loss=2.5196 tr_mae=2.099 | va_mae=3.440 va_rmse=4.799 | va_f1M=0.996 va_acc=0.995 | score=2.444 | time=23.8s\n",
            "[015] tr_loss=2.3668 tr_mae=1.985 | va_mae=3.292 va_rmse=4.602 | va_f1M=0.997 va_acc=0.997 | score=2.295 | time=23.6s\n",
            "[016] tr_loss=2.4259 tr_mae=2.033 | va_mae=3.160 va_rmse=4.424 | va_f1M=0.997 va_acc=0.997 | score=2.163 | time=22.7s\n",
            "[017] tr_loss=2.0429 tr_mae=1.752 | va_mae=3.034 va_rmse=4.259 | va_f1M=0.997 va_acc=0.997 | score=2.037 | time=23.5s\n",
            "[018] tr_loss=1.9042 tr_mae=1.653 | va_mae=2.908 va_rmse=4.088 | va_f1M=0.997 va_acc=0.997 | score=1.910 | time=24.0s\n",
            "[019] tr_loss=1.8631 tr_mae=1.627 | va_mae=2.792 va_rmse=3.931 | va_f1M=0.997 va_acc=0.997 | score=1.795 | time=23.9s\n",
            "[020] tr_loss=1.6428 tr_mae=1.460 | va_mae=2.673 va_rmse=3.776 | va_f1M=0.999 va_acc=0.998 | score=1.674 | time=22.9s\n",
            "[021] tr_loss=1.7078 tr_mae=1.511 | va_mae=2.567 va_rmse=3.637 | va_f1M=0.999 va_acc=0.998 | score=1.569 | time=23.3s\n",
            "[022] tr_loss=1.3920 tr_mae=1.282 | va_mae=2.460 va_rmse=3.496 | va_f1M=0.999 va_acc=0.998 | score=1.461 | time=23.5s\n",
            "[023] tr_loss=1.4142 tr_mae=1.299 | va_mae=2.361 va_rmse=3.363 | va_f1M=1.000 va_acc=1.000 | score=1.361 | time=23.8s\n",
            "[024] tr_loss=1.3356 tr_mae=1.234 | va_mae=2.264 va_rmse=3.232 | va_f1M=1.000 va_acc=1.000 | score=1.264 | time=23.0s\n",
            "[025] tr_loss=1.1806 tr_mae=1.127 | va_mae=2.178 va_rmse=3.116 | va_f1M=1.000 va_acc=1.000 | score=1.178 | time=23.6s\n",
            "[026] tr_loss=1.1493 tr_mae=1.098 | va_mae=2.097 va_rmse=3.008 | va_f1M=1.000 va_acc=1.000 | score=1.097 | time=23.7s\n",
            "[027] tr_loss=1.0174 tr_mae=0.995 | va_mae=2.016 va_rmse=2.904 | va_f1M=1.000 va_acc=1.000 | score=1.016 | time=23.3s\n",
            "[028] tr_loss=0.9785 tr_mae=0.954 | va_mae=1.938 va_rmse=2.801 | va_f1M=1.000 va_acc=1.000 | score=0.938 | time=22.7s\n",
            "[029] tr_loss=0.9343 tr_mae=0.927 | va_mae=1.871 va_rmse=2.709 | va_f1M=1.000 va_acc=1.000 | score=0.871 | time=23.8s\n",
            "[030] tr_loss=0.8214 tr_mae=0.830 | va_mae=1.808 va_rmse=2.622 | va_f1M=1.000 va_acc=1.000 | score=0.808 | time=23.7s\n",
            "[031] tr_loss=0.8098 tr_mae=0.814 | va_mae=1.756 va_rmse=2.553 | va_f1M=1.000 va_acc=1.000 | score=0.756 | time=23.1s\n",
            "[032] tr_loss=0.7401 tr_mae=0.746 | va_mae=1.703 va_rmse=2.482 | va_f1M=1.000 va_acc=1.000 | score=0.703 | time=22.9s\n",
            "[033] tr_loss=0.7277 tr_mae=0.744 | va_mae=1.657 va_rmse=2.419 | va_f1M=1.000 va_acc=1.000 | score=0.657 | time=23.9s\n",
            "[034] tr_loss=0.7070 tr_mae=0.713 | va_mae=1.614 va_rmse=2.360 | va_f1M=1.000 va_acc=1.000 | score=0.614 | time=23.7s\n",
            "[035] tr_loss=0.6994 tr_mae=0.708 | va_mae=1.579 va_rmse=2.313 | va_f1M=1.000 va_acc=1.000 | score=0.579 | time=23.5s\n",
            "[036] tr_loss=0.6048 tr_mae=0.636 | va_mae=1.545 va_rmse=2.269 | va_f1M=1.000 va_acc=1.000 | score=0.545 | time=23.2s\n",
            "[037] tr_loss=0.5658 tr_mae=0.579 | va_mae=1.516 va_rmse=2.234 | va_f1M=1.000 va_acc=1.000 | score=0.516 | time=24.0s\n",
            "[038] tr_loss=0.5485 tr_mae=0.563 | va_mae=1.487 va_rmse=2.201 | va_f1M=1.000 va_acc=1.000 | score=0.487 | time=23.7s\n",
            "[039] tr_loss=0.5132 tr_mae=0.520 | va_mae=1.459 va_rmse=2.171 | va_f1M=1.000 va_acc=1.000 | score=0.459 | time=24.6s\n",
            "[040] tr_loss=0.5042 tr_mae=0.512 | va_mae=1.436 va_rmse=2.149 | va_f1M=1.000 va_acc=1.000 | score=0.436 | time=23.8s\n",
            "[041] tr_loss=0.4836 tr_mae=0.488 | va_mae=1.413 va_rmse=2.126 | va_f1M=1.000 va_acc=1.000 | score=0.413 | time=23.6s\n",
            "[042] tr_loss=0.4634 tr_mae=0.457 | va_mae=1.393 va_rmse=2.108 | va_f1M=1.000 va_acc=1.000 | score=0.393 | time=24.4s\n",
            "[043] tr_loss=0.4301 tr_mae=0.419 | va_mae=1.371 va_rmse=2.086 | va_f1M=1.000 va_acc=1.000 | score=0.371 | time=24.0s\n",
            "[044] tr_loss=0.4216 tr_mae=0.408 | va_mae=1.353 va_rmse=2.070 | va_f1M=1.000 va_acc=1.000 | score=0.353 | time=23.7s\n",
            "[045] tr_loss=0.4226 tr_mae=0.412 | va_mae=1.336 va_rmse=2.058 | va_f1M=1.000 va_acc=1.000 | score=0.336 | time=23.3s\n",
            "[046] tr_loss=0.4031 tr_mae=0.385 | va_mae=1.319 va_rmse=2.044 | va_f1M=1.000 va_acc=1.000 | score=0.319 | time=24.0s\n",
            "[047] tr_loss=0.4013 tr_mae=0.377 | va_mae=1.304 va_rmse=2.033 | va_f1M=1.000 va_acc=1.000 | score=0.304 | time=23.8s\n",
            "[048] tr_loss=0.3865 tr_mae=0.359 | va_mae=1.290 va_rmse=2.024 | va_f1M=1.000 va_acc=1.000 | score=0.290 | time=23.8s\n",
            "[049] tr_loss=0.3729 tr_mae=0.336 | va_mae=1.278 va_rmse=2.014 | va_f1M=1.000 va_acc=1.000 | score=0.278 | time=22.9s\n",
            "[050] tr_loss=0.3615 tr_mae=0.319 | va_mae=1.267 va_rmse=2.004 | va_f1M=1.000 va_acc=1.000 | score=0.267 | time=23.5s\n",
            "[051] tr_loss=0.3613 tr_mae=0.317 | va_mae=1.257 va_rmse=1.998 | va_f1M=1.000 va_acc=1.000 | score=0.257 | time=24.0s\n",
            "[052] tr_loss=0.3569 tr_mae=0.303 | va_mae=1.247 va_rmse=1.993 | va_f1M=1.000 va_acc=1.000 | score=0.247 | time=23.8s\n",
            "[053] tr_loss=0.3581 tr_mae=0.304 | va_mae=1.239 va_rmse=1.987 | va_f1M=1.000 va_acc=1.000 | score=0.239 | time=23.1s\n",
            "[054] tr_loss=0.3471 tr_mae=0.295 | va_mae=1.230 va_rmse=1.981 | va_f1M=1.000 va_acc=1.000 | score=0.230 | time=23.0s\n",
            "[055] tr_loss=0.3491 tr_mae=0.296 | va_mae=1.223 va_rmse=1.976 | va_f1M=1.000 va_acc=1.000 | score=0.223 | time=23.9s\n",
            "[056] tr_loss=0.3539 tr_mae=0.296 | va_mae=1.217 va_rmse=1.972 | va_f1M=1.000 va_acc=1.000 | score=0.217 | time=23.8s\n",
            "[057] tr_loss=0.3471 tr_mae=0.286 | va_mae=1.211 va_rmse=1.968 | va_f1M=1.000 va_acc=1.000 | score=0.211 | time=23.2s\n",
            "[058] tr_loss=0.3434 tr_mae=0.281 | va_mae=1.205 va_rmse=1.965 | va_f1M=1.000 va_acc=1.000 | score=0.205 | time=22.8s\n",
            "[059] tr_loss=0.3444 tr_mae=0.284 | va_mae=1.200 va_rmse=1.963 | va_f1M=1.000 va_acc=1.000 | score=0.200 | time=24.5s\n",
            "[060] tr_loss=0.3430 tr_mae=0.281 | va_mae=1.196 va_rmse=1.961 | va_f1M=1.000 va_acc=1.000 | score=0.196 | time=24.1s\n",
            "\n",
            "[Test] mae=1.144 rmse=1.972 | f1M=0.997 acc=0.997\n",
            "\n",
            "[완료] 모델/기록/그래프 저장: convnext_tiny_multi_ckpt\n"
          ]
        }
      ]
    }
  ]
}